Final projects must meet the following requirements:

Explain roles and responsibilities of all team members.
Create a new Github repo for the project! Check in all your project work into Github.
Use Markdown (with structure) for your project report and all documentation.
Define one or more research questions (RQs) that motivate/drive data collection and analysis.
Acquire at least two different datasets (cf. Weeks 3-4)
When using an "data hub" like Kaggle, make sure to trace/go back to the original sources if possible.
Either one dataset must come from an API or datasets must use different format/schema.
If datasets use different licenses, report how you deal with this.
Datasets should be acquired programmatically with integrity checks (checksums). If not possible, steps to acquire data must be well-documented (reproducible)
Do not push datasets to your project repository (your scripts will access data via API or download at execution time)
Automatic (programmatic) integration of datasets using Python/Pandas and/or SQL (cf. Weeks 5-6)
Document data profiling, quality assessment, and cleaning (cf. Week 7)
Implement simple data analysis and/or visualization (answering at least part of your RQs)
Create a reproducible package (cf. Week 8)
Automated end-to-end workflow execution (cf. Week 10)
Accurate citation of data and software used (cf. Weeks 11-12)
Create metadata describing your package (cf. Week 13)
Archive your project in a repository, obtaining a persistent identifier (cf. Week 14)